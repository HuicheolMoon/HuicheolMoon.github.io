---
layout: post
title: "P stage (2) : 1. 인공지능과 자연어 처리"
category: P_stage
date: 2021-04-12 19:00:00 +0900
---
### 목표
1. 강의 수강 및 이해
2. 대략적인 대회 코드 이해 및 수정

### 강의 학습
자연어 처리의 시작은 앨런 튜링이 설계한 튜링 테스트라고 할 수 있다. 튜링 테스트는 기계에 인간과 같은 지능이 있는지 판별하고자 하는 시험인데 여기서는 인간과 같은 지능을 '인간이 보기에 인간같은 것'으로 정의한다. 어떤 대상이 인간인지 아닌지를 판별하기 위해서는 그 대상과의 상호작용이 필요했고 이를 위한 인간과 컴퓨터의 상호작용을 구현한 것이 자연어 처리의 시작이다.
현재 NLP는 기계 독해나 챗봇에서부터 가설 검증이나 이미지 캡셔닝 등 매우 다양한 Domain에서 사용되는 기술이다. 앞서 말했듯이 NLP는 궁극적으로 인간과 컴퓨터의 상호작용을 구현하기 위한 기술이기 때문에 인간이 발화하는 자연어를 컴퓨터가 처리할 수 있게 변환하는 작업, Encoding은 매우 중요한 역할을 맡고 있다. 대부분의 NLP 문제는 Classification의 문제로 치환할 수 있는데 이는 인간의 자연어를 벡터로 인코딩하고 수학적인 틀 안에서 분류함으로서 해결할 수 있다. 분류를 위해서는 자연어 데이터의 특징을 파악해야한다. 인간은 듣는 단어를 사전지식을 통해 대상의 크기나 색 등으로 분류할 수 있지만 컴퓨터는 아직 그럴 수 없다. 그렇기에 input data를 각 feature마다 특정한 값을 가지게 하고 이 값들을 모은 vector를 그래프 위에 표현한다면 컴퓨터가 사전 정의한, 혹은 학습한 분류 경계를 통해 데이터를 분류할 수 있다. 과거에는 사람이 직접 feature를 파악해서 분류했지만 실제 복잡한 문제들에선 분류 대상의 특징을 사람이 파악하기 어려울 수 있다. 그렇기 때문에 컴퓨터가 스스로 특징을 찾고 분류할 필요가 있는데 이것이 기계학습의 핵심이다.

#### One-hot encoding
자연어를 vector로 표현하는 방법 중 가장 단순한 것은 one-hot encoding 방식이다. 가령 input data [A, B, C]가 주어진다면 이를 A, B, C의 3 가지 class로 분류하여 vector (A,B,C)를 이용해 {A:(1,0,0), B:(0,1,0), C:(0,0,1)}의 꼴로 나타내는 것이다. 이 방법은 단순하지만 vector가 sparse해서 단어가 가지는 의미를 벡터 공간에 표현할 수 없고, input의 class가 증가하면 vector의 dimension도 증가하는 단점을 가지고 있다.

#### Word2Vec
이를 해결하기 위한 알고리즘이 Word2Vec이다. Word2Vec은 한 단어의 주변 단어들을 통해 그 단어의 의미를 파악하는 Skip-gram 방식을 사용한다. Word2Vec을 통해 얻은 자연어 dense vector는 자연어가 가지는 의미를 vector space에 표현할 수 있다. 이는 다른 두 단어 간의 유사도와 관계를 수학적으로 계산하고 나아가서 단어의 의미를 연산을 통해 추론할 수 있게 한다. 하지만 단어의 subword information을 포함할 수 없고 OOV(Our of vocabulary)에는 적용할 수 없다는 단점이 있다.

#### FastText
FastText는 단어의 다양한 subword 간의 관계를 구현할 수 있다. 기본적인 틀은 Word2Vec와 유사하지만 단어 학습 시, 단어를 n-gram으로 나누어 학습하여 입력 단어가 vocabulary에 있을 경우, word2vec과 마찬가지로 해당 단어의 word vector를 return하고 OOV일 경우 입력 단어의 n-gram vector들의 합산을 return하여 subword 간 의미 유사도를 보존할 수 있다. 하지만 여전히 주변 단어를 통해 학습이 이루어지므로 문맥을 고려할 수 없고 동형어, 다의어 등에 성능이 좋지 못하다는 단점이 존재한다.

#### Language Model
NLP 언어모델은 주어진 단어들로부터 그 다음에 등장한 단어의 확률을 예측하는 방식으로 학습하여 문맥을 포함하는 정보를 제공할 수 있게 한다. Markcov chain model 등 초기의 언어모델은 단순히 다음의 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산했다. 하지만 RNN이 등장하면서 앞선 단어들의 ‘문맥’을 고려해서 만들어진 최종 Context vector를 classification layer를 통해 분류하는 NN model이 완성되었다.

#### Seq2Seq
Seq2Seq는 더 나아가서 RNN 구조를 통해 Context vector 를 획득하는 Encoder layer와 획득된 Context vector를 입력으로 출력을 예측하는 Decoder layer로 model을 구성하여 성능을 대폭 끌어올리고 사용할 수 있는 분야도 확장시켰다. 하지만 입력 sequence의 길이가 매우 긴 경우 처음에 나온 token에 대한 정보가 희석되고 고정된 context vector 사이즈로 인해 긴 sequence에 대한 정보를 함축하기 어렵다는 아쉬움을 보였다.

#### Attention & Transfomer
이를 해결하기 위해 등장한 알고리즘이 Attention이다. 인간이 정보처리를 할 때 모든 sequence를 고려하면서 정보처리를 하는 것이 아니듯 중요한 feature는 더욱 중요하게 고려하는 것이 Attention의 모티브이다. 문맥에 따라 동적으로 할당되는 encode의 Attention weight로 인한 dynamic context vector를 획득하여 기존 Seq2Seq의 encoder, decoder 성능을 비약적으로 향상시켰다. 여전히 RNN의 구조적 문제로 인한 연산 속도의 한계가 존재했는데 이를 해결하기 위해 RNN 없이 Attention과 layer로만 구성한 model이 Transfomer다. 현재 대부분의 language model이 transformer 기반일 만큼 transformer는 modern NLP 그 자체라고 생각한다. Transfomer는 기존 Seq2Seq의 encoders와 decoders가 LSTM으로 구성된 RNN이 아니라 낱개의 encoder와 decoder가 각각 쌓아올려지고 각 층마다 Attention 구조를 통해 input sentense를 병렬적으로 처리할 수 있다. 현재도 Transfomer 기반의 bidirectional LM인 BERT(Google)를 기반으로 RoBERTa(Facebook) 등 여러 LM 들이 개발되고 있다. Transformer의 기술적인 내용에 대해서는 아래 링크를 참조하면 좋다.
[NLP in Korean - Transformer](https://nlpinkorean.github.io/illustrated-transformer)

### Daily mission
1. 자연어처리 기술이 적용될 수 있는 서비스 application의 아이디어를 제안해주세요!
AI NLP 어플리케이션 아이디어를 제출하는 과제였다. 다들 좋은 내용들을 제안해서 고르기 힘들었다. 결국 유튜버 QnA 챗봇을 선택했다.

### Competition
기본 코드를 가지고 default submission 진행했다.
1. .to_csv()에서 argument로 csv 파일명을 넣을 때 경로 폴더는 미리 만들어야 함. if not -> FileNotFoundError 발생
2. 학습이 진행될수록 정확도가 크게 향상 (checkpoint 1500:56.0%, 2000:58.1%) -> train data 크기가 작아서 overfitting 등의 효과는 거의 없다고 봐도 될 듯 -> data augmentation 등 어떻게든 train data를 늘릴 필요가 있음

### 학습 소감
- 가장 흥미가 있던 분야인 만큼 미친듯이 해서 좋은 결과를 얻으리라.

<br/>

### References
1. NAVER Connect Foundation: Boostcamp AI Tech
