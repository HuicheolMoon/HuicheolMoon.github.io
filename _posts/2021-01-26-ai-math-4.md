---
layout: post
title: "AI Math : 4. Gradient descent (1)"
category: DL
date: 2021-01-26 19:00:00 +0900
---
## Intro
>미분의 개념과 그래디언트 벡터, 경사하강법의 알고리즘에 대해 공부합니다.

## Differentiation
미분(differentiation)은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 제일 많이 사용하는 기법입니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/108038883-b6344080-707e-11eb-854b-f5ebe54226a8.png" alt="differentiation"/>
  differentiation
</p>

## Gradient descent

미분은 함수 f의 주어진 점(x, f(x))에서의 접선의 기울기를 구하는 것과 같습니다. 이 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가 또는 감소하는지 알 수 있습니다. 이렇게 해당 지점에서 접선의 기울기를 이용하여 해당 function의 극소값을 구하는 알고리즘을 경사하강법(Gradient descent)이라고 합니다. 변수가 벡터인 경우에도 각 성분에 대한 편미분을 통해 경사하강법을 적용할 수 있습니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/108039244-32c71f00-707f-11eb-8a5f-aa93f539ff72.png" alt="Vector Gradient descent"/>
  Vector Gradient descent
</p>

<br/>

### References
1. NAVER Connect Foundation: Boostcamp AI Tech
