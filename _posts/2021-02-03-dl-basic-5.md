---
layout: post
title: "DL Basic : 5. Modern Convolutional Neural Networks"
category: DL
date: 2021-02-03 19:10:00 +0900
---
## Intro
>ILSVRC라는 Visual Recognition Challenge와 대회에서 수상했던 5개 Network의 주요 아이디어와 구조에 대하여 공부합니다.

## ILSVRC
ILSVRC는 ImageNet Large-Scale Visual Recognition Challenge의 약자로 이미지 인식 대회입니다. 여기서 이미지 인식은 단순 Detection뿐만 아니라 Classification, Localization, Segmentation을 모두 포함합니다. 1000개의 다양한 카테고리와 백만개 이상의 이미지를 456,567개의 train images를 학습시켜 처리하게 됩니다. 2015년 이후로 ILSVRC에서 수상한 모델은 인간보다 높은 정확성을 보이고 있습니다.

## AlexNet
AlexNet은 2012년에 우승한 CNN입니다. AlexNet은 ReLU를 활성함수로 사용하며 Local response normalization, Overlapping pooling, Dataaugmentation, Dropout 방법을 사용하여 모델의 효율성과 정확성을 올렸습니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/108181006-bd288500-714a-11eb-8d39-cae960630fd0.png" alt="AlexNet"/>
  AlexNet
</p>

## VGGNet
VGGNet은 2014년에 준우승한 CNN입니다. stride가 1인 convolution filter를 이용하여 네트워크를 더욱 깊게 만들었습니다. VGGNet의 논문에서는 네트워크가 깊어질수록 성능이 좋아진다는 것을 보여줍니다. 같은 size의 특성 맵을 출력할 때 깊이가 깊어질수록 필요한 파라미터의 개수가 줄어들기 때문에 학습 속도가 빠르다는 장점이 있습니다.

## GoogLeNet
GoogLeNet은 2014년에 우승한 CNN입니다. GoogLeNet은 inception block을 통해 parameter의 개수를 줄였습니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/108182269-2c52a900-714c-11eb-983a-5ae78bd5d47f.png" alt="GoogLeNet"/>
  GoogLeNet
</p>

## ResNet
ResNet은 2015년에 우승한 CNN입니다. ResNet은 152개의 층을 가지고 있고 이는 GoogLeNet보다 7배나 깊습니다. CNN의 층이 많아진다고 모델의 성능이 무조건 좋아지는 것은 아닙니다. 이를 해결하기 위해 ResNet은 Residual Block을 도입했습니다. Residual Block은 입력값을 출력값에 더해줄 수 있도록 지름길(shortcut)을 덧붙인 구조입니다.

## DenseNet
DenseNet은 ResNet과 달리 모든 이전 레이어의 output 정보를 이후의 레이어의 input으로 받아오는 방법을 사용합니다. 이 때 DenseNet은 모든 정보를 더하는 대신 concatenation을 사용하여 처리합니다.

<br/>

### References
1. NAVER Connect Foundation: Boostcamp AI Tech
2. ImageNet Classification with Deep Convolutional Neural Networks, NIPS, Alex et al.
3. Going Deeper with Convolutions, CVPR, Christian et al.
