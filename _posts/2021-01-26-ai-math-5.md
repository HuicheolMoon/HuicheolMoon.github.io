---
layout: post
title: "AI Math : 5. Gradient descent (2)"
category: DL
date: 2021-01-26 19:10:00 +0900
---
## Intro
>경사하강법 기반의 선형회귀 알고리즘에 대해 공부합니다. 그리고 경사하강법의 단점을 보완하는 확률적 경사하강법(stochastic gradient descent)을 공부합니다.

## Gradient descent with linear regression
np.linalg.pinv를 이용하면 데이터를 선형 모델(linear model)로 해석하는 선형회귀식을 찾을 수 있습니다. 선형회귀의 목적식을 최소화하는 계수를 찾는 데에 경사하강법을 적용할 수 있습니다.

## Stochastic gradient descent
이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대하여 적절한 학습률과 학습 횟수를 선택했을 때 수렴이 보장되어 있습니다. 하지만 비선형회귀문제의경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않습니다. 이를 해결하기위해 확률적 경사하강법(stochasticgradientdescent)을 도입할 수 있습니다. 확률적 경사하강법은 모든 데이터를 사용해서 업데이트하는 대신 데이터의 일부만을 활용하여 업데이트합니다. SGD는 데이터의 일부를 가지고 parameter를 업데이트하기 때문에 자원을 좀 더 효율적으로 활용하는 데에 도움이 됩니다. 또한 미니배치를 확률적으로 선택하므로 목적식 모양이 계속 바뀌게 되어 볼록이 아닌 목적식에서도 사용가능하고, 경사하강법보다 머신러닝에 더 효율적입니다.

<br/>

### References
1. NAVER Connect Foundation: Boostcamp AI Tech
