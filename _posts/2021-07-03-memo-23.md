---
layout: post
title: "Today I learned 21.07.03"
category: MEMO
date: 2021-07-03 19:00:00 +0900
---
## CS

### 1. OS

1. 메모리 관리 전략
    - Swapping : 메모리의 관리를 위해 사용되는 기법. 표준 Swapping 방식으로는 round-robin 과 같은 스케줄링의 다중 프로그래밍 환경에서 CPU 할당 시간이 끝난 프로세스의 메모리를 보조 기억장치(e.g. 하드디스크)로 내보내고 다른 프로세스의 메모리를 불러 들일 수 있다.
    - 단편화 (Fragmentation) : 프로세스들이 메모리에 적재되고 제거되는 일이 반복되면, 프로세스들이 차지하는 메모리 틈 사이에 사용하지 못할 만큼의 작은 자유공간들이 늘어남.
        1. 외부 단편화: 메모리 공간 중 사용하지 못하게 되는 일부분. 물리 메모리(RAM)에서 사이사이 남는 공간.
        2. 내부 단편화: 프로세스가 사용하는 메모리 공간 내의 남는 부분.
    - 가상 메모리
        - 프로그램의 일부분만 메모리에 올려서 물리 메모리 크기에 제약받지 않고 더 많은 프로그램을 동시에 실행하면서 응답시간은 유지하고, CPU 이용률과 처리율을 높이는 기술. swap cost도 줄어듬.
        - 물리 메모리와 논리 메모리의 분리. 프로세스의 일부만 메모리에 로드하고 나머지는 보조기억장치에 두는 형태. 메모리 관리 장치(MMU)에 의해 물리 주소로 변환되어 사용자가 메모리 맵핑이 어떻게 되는지 의식할 필요 없이 알아서 가상 메모리를 활용하여 작업.
        - `시스템 라이브러리`가 여러 프로세스들 사이에 공유될 수 있도록 한다. 각 프로세스들은 `공유 라이브러리`를 자신의 가상 주소 공간에 두고 사용하는 것처럼 인식하지만, 라이브러리가 올라가있는 `물리 메모리 페이지`들은 모든 프로세스에 공유되고 있다.
        - 프로세스들이 메모리를 공유하는 것을 가능하게 하고, 프로세스들은 공유 메모리를 통해 통신할 수 있다. 이 또한, 각 프로세스들은 각자 자신의 주소 공간처럼 인식하지만, 실제 물리 메모리는 공유되고 있다.
        - `fork()`를 통한 프로세스 생성 과정에서 페이지들이 공유되는 것을 가능하게 한다.
    - **Demand Paging(요구 페이징)**
        - 프로그램 실행 시작 시에 프로그램 전체를 디스크에서 물리 메모리에 적재하는 대신, 초기에 필요한 것들만 적재하는 전략. 가상 메모리에서는 실행과정에서 필요해질 때 페이지들이 적재된다. 프로세스 내의 개별 페이지들은 `페이저(pager)`에 의해 관리된다. 페이저는 프로세스 실행에 실제 필요한 페이지들만 메모리로 읽어 옮으로서, **사용되지 않을 페이지를 가져오는 시간낭비와 메모리 낭비를 줄일 수 있다.**
    1. **Paging(페이징)**
        - 프로세스의 주소 공간을 동일한(고정된) 사이즈의 페이지 단위로 나누어 물리적 메모리에 불연속적으로 저장하는 방식. Page Table로 mapping.
        - 물리 메모리는 Frame 이라는 고정 크기로 분리, 논리 메모리는 페이지라 불리는 고정 크기의 블록으로 분리.
        - 장점 : 논리 메모리는 물리 메모리에 저장될 때, 연속되어 저장될 필요가 없고 물리 메모리의 남는 프레임에 적절히 배치됨으로 외부 단편화를 해결
        - 단점 : 내부 단편화 문제의 비중이 늘어남. (외부 단편화에 비해 미미함.)
    2. **Segmentation(세그멘테이션)**
        - 서로 다른 크기의 논리적 단위인 세그먼트(Segment)로 분할. 사용자가 두 개의 주소로 지정(세그먼트 번호 + 변위)
        - 세그먼트 테이블에는 각 세그먼트의 기준(세그먼트의 시작 물리 주소)과 한계(세그먼트의 길이)를 저장
        - 장점 : 메모리의 보호와 공유가 효율적.
        - 단점 : 외부 단편화 - 서로 다른 크기의 세그먼트들이 메모리에 적재되고 제거되는 일이 반복되다 보면, 자유 공간들이 많은 수의 작은 조각들로 나누어져 못 쓰게 될 수도 있다.
    3. Paged Segmentation
        - 페이징과 세그멘테이션의 장점 융합 → 세그멘테이션된 세그먼트들을 다시 페이징하여 페이지로 분할.

1. 페이지 교체 알고리즘
    - 프로그램 실행시에 모든 항목이 물리 메모리에 올라오지 않기 때문에, 프로세스의 동작에 필요한 페이지를 요청하는 과정에서 원하는 페이지를 보조저장장치에서 가져오게 된다. 하지만, 만약 물리 메모리가 모두 사용중인 상황이라면, 페이지 교체가 이뤄져야 한다.
    - 기본적인 페이지 교체 방법
        1. 디스크에서 필요한 페이지의 위치를 찾는다
        2. 빈 페이지 프레임을 찾는다.
            1. `페이지 교체 알고리즘`을 통해 희생될(victim) 페이지를 고른다.
            2. 희생될 페이지를 디스크에 기록하고, 관련 페이지 테이블을 수정한다.
        3. 새롭게 비워진 페이지 테이블 내 프레임에 새 페이지를 읽어오고, 프레임 테이블을 수정한다.
        4. 사용자 프로세스 재시작
    1. FIFO(First in First out) 알고리즘
        - 페이지 교체시 메모리에 먼저 올라온 페이지를 먼저 내보내는 알고리즘
        - 가장 간단한 방법으로, 특히 초기화 코드에서 적절한 방법이다.
        - 단점
            - 페이지의 향후 참조 가능성을 고려하지 않고, 물리적 메모리에 들어온 순서대로 내쫓을 대상을 선정하기 때문에 비효율적인 상황이 발생할 수 있다.
            - 벨레이디의 모순 현상이 발생할 수 있다. (페이지 프레임 수가 많으면(메모리 증가) 페이지 부재수가 줄어드는 것이 일반적이지만, 페이지 프레임 수를 증가시켰음에도 페이지 부재가 더 많이 일어나는 현상을 의미한다.)

                ![Belady's Anomaly](https://user-images.githubusercontent.com/77161691/126346440-a6f89e19-2804-4230-afd9-5e96eccedec5.png)

                ([https://en.wikipedia.org/wiki/B%C3%A9l%C3%A1dy%27s_anomaly](https://en.wikipedia.org/wiki/B%C3%A9l%C3%A1dy%27s_anomaly))

    2. 최적 페이지 교체 알고리즘(Optimal Page Replacement)
        - 앞으로 가장 오랫동안 사용하지 않을 페이지를 교체.
        - 비현실적 - 미래에 어떤 페이지가 어떠한 순서로 참조될지 미리 알고있다는 전제가 필요
    3. LRU(Least Recently Used) 알고리즘
        - 최근에 사용하지 않은 페이지를 가장 먼저 내보내는 방법
        - **실제로 사용할 수 있는 페이지 교체 알고리즘에서는 가장 좋은 방법 중 하나.**
    4. LFU 알고리즘(Least Frequently Used)
        - 물리적 메모리 내에 존재하는 페이지 중 과거에 참조 횟수가 가장 적었던 페이지를 내보내는 방법.
        - 시간에 따른 페이지 참조의 변화를 반영하지 못하고 LRU보다 구현이 복잡하다. 오버헤드도 존재
    5. 클럭 알고리즘(NUR : Not Used Recently)
        - 하드웨어적인 지원을 받아 LRU를 근사시킨 알고리즘. 오랫동안 사용하지 않은 페이지 중 하나를 교체.
        - 환형 리스트 & 참조 비트(Reference Bit)와 변형 비트(Modified Bit, Dirty Bit)를 사용.
            - 참조 비트 : 페이지가 참조될 때 1로 세팅. 다시 페이지를 만나면 0으로. 참조비트가 0인 페이지를 만나면 교체.
        - 대부분의 시스템에서 클럭 알고리즘을 채택. (하드웨어의 자원으로 동작하기 때문에 LRU에 비해 교체 페이지의 선정이 훨씬 빠르게 결정)

2. 캐시
    1. 캐시의 지역성(locality)
        - 캐시 메모리는 장치간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리이다. 이러한 역할을 수행하기 위해서는 CPU 가 어떤 데이터를 원할 것인가를 어느 정도 예측할 수 있어야 한다. 이 때, `적중율(Hit rate)`을 극대화 시키기 위해 데이터 `지역성(Locality)의 원리`를 사용한다. 지역성의 전제조건으로 프로그램은 모든 코드나 데이터를 균등하게 Access 하지 않는다는 특성을 기본으로 한다. 즉, `Locality`란 기억 장치 내의 정보를 균일하게 Access 하는 것이 아닌 어느 한 순간에 특정 부분을 집중적으로 참조하는 특성인 것이다.
        - 데이터 지역성
            - 시간 지역성 : 최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성.
            - 공간 지역성 : 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성
        - Caching line
            - 캐시에 목적 데이터가 저장되어 있다면 바로 접근하여 출력할 수 있어야 캐시가 의미 있다. 그렇기 때문에 캐시에 데이터를 저장할 때 특정 자료구조를 사용하여 캐싱라인 묶음으로 저장. 메모리 주소 등을 태그로 저장. 메모리에서 캐시로 데이터를 가져올 때도 캐싱 라인 단위로 가져온다.
            1. Full Associative : 태그 일치 여부를 병렬로 확인 (칩도 넓어야 하고, 전력도 많이 필요)
            2. Direct Map : 메모리 주소의 일부를 태그로 사용
            3. Set Associative : Direct Map 방식을 중첩 + Multiplexer와 OR gate 등 사용
