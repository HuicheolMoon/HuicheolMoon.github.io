---
layout: post
title: "AI Math : 7. Neural Network"
category: DL
date: 2021-01-27 19:10:00 +0900
---
## Intro
>신경망의 구조와 내부에서 사용되는 softmax, 활성함수, 역전파 알고리즘에 대해 공부합니다.

## Neural Network
선형모델은 단순한 데이터를 해석할 때는 유용하지만 분류문제나 좀 더 복잡한 패턴의 문제를 풀 때는 예측성공률이 높지 않습니다. 이를 개선하기 위한 비선형 모델이 신경망(Neural Network)입니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/108044446-9c4a2c00-7085-11eb-8697-e1b45b0de6b6.png" alt="Neural Network Equation"/>
  Neural Network Equation
</p>

신경망에서 각 행벡터는 데이터셋과 가중치행렬을 곱한 후 임의의 절편벡터와의 합으로 표현됩니다. 이 행벡터를 활성함수와 합성하여 다시 새로운 잠재벡터를 만들게 됩니다. 활성함수(activation function)는 딥러닝을 위한 비선형(non-linear)함수로서 매우 중요한 개념입니다. 활성함수가 존재하지 않으면 딥러닝은 선형 모형과 차이가 없습니다. 예전에는 Sigmoid나 tanh가 많이 사용되었지만 최근에는 ReLU가 많이 사용되고 있고 계속 새로운 활성함수들이 제안되고 있습니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/108045218-88eb9080-7086-11eb-89a2-ec0e1ed706c1.png" alt="activation function"/>
  Activation Function
</p>

그리고 이렇게 행벡터와 활성함수가 합성된 신경망을 여러 층 쌓아올린 구조를 다층 퍼셉트론(MLP)라고 부릅니다. 이론적으로는 얇은 신경망으로도 목적함수를 근사하는 것이 가능하지만 층이 깊어질수록 목적함수를 근사하는데 필요한 노드의숫자가 훨씬 줄어들기 때문에 일반적인 경우에는 많은 층을 쌓아서 더 효율적인 학습을 진행합니다.

## Softmax
각 행벡터는 다양한 범위의 값을 가지고 있는데 이 출력 벡터를 softmax 함수를 통해 정규화시키면 출력벡터를 결과가 특정 클래스에 속할 확률로 해석할 수 있게 됩니다. softmax는 분류 문제를 해결할 때 자주 사용되는 연산입니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/108044365-82a8e480-7085-11eb-95eb-6d1b508de1e5.png" alt="Pandas Dataframe"/>
  Pandas Dataframe
</p>

## Backpropagation
역전파(backpropagation) 알고리즘은 각 층의 패러미터의 그레디언트 벡터를 윗층부터 역순으로 계산하여 학습의 결과를 가지고 parameter를 학습하는 알고리즘입니다. 역전파 알고리즘은 미분의 연쇄법칙(chain-rule)을 이용하여 각 단계에서의 편미분값을 곱하고 이를 결과값에 곱하여 각 단계마다 parameter가 어떻게 변화해야 목적함수가 최소화되는지에 대한 단서를 제공해줍니다.

<br/>

### References
1. NAVER Connect Foundation: Boostcamp AI Tech
