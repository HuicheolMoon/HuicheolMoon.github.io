---
layout: post
title: "P stage (3) : 3. BERT 언어 모델 (1)"
category: P_stage
date: 2021-04-14 19:00:00 +0900
---
### 목표
1. 강의 수강 및 이해
2. 대략적인 대회 코드 이해 및 수정

### 강의 학습
#### BERT
BERT(Bidirectional Encoder Representations from Transformers)는 Google이 개발한 NLP model이다. 현재 대부분의 NLP model이 BERT를 기반으로 할 만큼 획기적인 모델이다.

기존의 Seq2Seq는 데이터를 압축하는 encoder와 데이터를 복원하는 decoder로 구성되었다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/77161691/114691642-da489100-9d52-11eb-90de-8938dec11c35.png" alt="BERT"/>
  BERT
</p>

BERT는 input data에 masking이 있고 이를 바탕으로 완전한 원래 data를 추론하는 모델이다.

* input : sentence 2개 ([SEP] token으로 분리) // [CLS] (sent1) [SEP] (sent2) //

* transformer layer : 12개 layer가 all-to-all network
	- [CLS]는 sent1,2를 표현하는 정보를 가지고 있음

* tokenizer : WordPiece tokenizing - 빈도수 기반

1. sent1 선택 후 원래 다음 sent가 올 확률 50% / 아닌 다른 random sent가 올 확률 50%
2. masking : [CLS]와 [SEP]를 제외한 token 중 15% 확률로 token을 선택하여 masking 확률 80%, random replacing 확률 10%, unchanging 확률 10%로 변환 -> input data 완성
3. 여러 task를 통해 모델 평가

- tokenizer units
    - CV : 자소
    - Syllable : 음절
    - Morpheme : 형태소
    - Morpheme-aware Subword : 형태소 분석 후 wordpiece

#### 한국어 BERT model
 - ETRI KoBERT - 기존 model과 달리 형태소 기반 언어모델 분석을 통해 한국어 특화 but ETRI 형태소 분석기를 사용해야 함.
 - SKT KoBERT - 기존 wordpiece tokenizer 사용

KBQA에서 가장 중요한 entity정보가 기존 BERT에서는 무시된다. 그래서 Entity linking을 통해 주요 entity 추출 및 entitytag 부착작업을 수행하고 Entity embedding layer 추가한다. 마지막으로 형태소 분석을 통해 NNP와 entity 우선 chunking masking을 수행한다. [ENT] tag를 이용할 경우 한국어 처리 성능 상승한다. -> ERNIE model과 비슷

### Daily mission
#### QnA data를 이용하여 유사도 기반 챗봇 구현
[data preprocessing sudo code (Colab)]

```python
# 1. google drive mount
from google.colab import drive
drive.mount('/content/drive')

# 2. data download from [data_url]
!git clone [data_url]

# 3. csv data를 읽어오기
import pandas as pd
chatbot_data = pd.read_csv("[csv file location]")

# csv file location은 colab 좌측 탭에서 google drive 경로 복사 기능 사용

# 4. pandas table을 list로 편집
max_len = len(chatbot_data)
chatbot_Question = []
chatbot_Answer = []
for i in range(max_len):
    chatbot_Question.append(chatbot_data.loc[i,'Q'])
    chatbot_Answer.append(chatbot_data.loc[i,'A'])
```

이후 input question에 대해 chatbot_Question에 있는 질문들과의 코사인 유사도를 비교하여 제일 높은 질문에 해당하는 답을 chatbot_Answer에서 찾아 response하는 알고리즘이다.

1. 실제 챗봇에 적용하기에는 어떤 문제점이 있을까요? 🤗 고민해주세요!
 - 저장된 QnA list의 크기가 충분히 크지 않으면 OOV question에 대한 성능이 많이 떨어질 것
 - 그렇다고 QnA list의 크기를 무작정 키우기에는 필요한 resource도 많을 뿐더러 저장된 모든 질문들과의 코사인유사도를 계산하고 비교하는 과정이 진행되기 때문에 QnA list가 커질수록 시간도 오래 걸린다.
 - 근본적으로 코사인 유사도가 높아도 인간 입장에서 질문의 의미가 비슷하지 않을 수 있음. (인간의 관점) 이는 OOV 경우에서는 더욱 도드라질 것.

### Competition
기본 코드를 가지고 default submission을 진행했다.
 - batch size를 극단적으로 줄이니(16 -> 1) 오히려 정확도가 감소하는 결과가 나타났다. 중간값인 8, 4, 2 등에 대해 각각 학습을 진행해서 경향성을 살펴봐야할 필요가 있다.

### 학습 소감
- 데이터 전처리 작업에서 pandas 다루는 능력이 중요하다 - 공부하자
- tqdm은 정말 좋은 tool이다. 조금이라도 오래 걸릴 것 같으면 tqdm을 사용해야 잘되고있는지 망했는지 바로 알 수 있다. 인간은 작업이 얼마나 남았는지 모르면 불안해서 버틸 수 없다. tqdm은 노벨 평화상을 받아야 한다.
- 한국어는 어절 단위보다 음절 단위 tokenizing이 더 효과적인가? - 더 고민해보자

<br/>

### References
1. NAVER Connect Foundation: Boostcamp AI Tech
2. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] (https://arxiv.org/pdf/1810.04805)
